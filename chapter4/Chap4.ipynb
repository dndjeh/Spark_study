{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6531d690",
   "metadata": {},
   "source": [
    "<h2>미국 항공편 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b977198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "schema = \"'date' STRING, 'delay' INT, 'distance' INT, 'origin' STRING, 'destination' STRING\"\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName('SparkSQLExampleApp')\n",
    "         .getOrCreate())\n",
    "\n",
    "filepath = 'departuredelays.csv'\n",
    "\n",
    "df = (spark.read.format('csv')\n",
    "      .option('inferSchema','true') #데이터의 타입(숫자, 문자열 등)을 자동으로 추론하도록 설정합니다.\n",
    "      .option('header','true') \n",
    "      .load(filepath))\n",
    "\n",
    "df.createOrReplaceTempView('us_delay_flights_tbl') #임시뷰를 만드는 함수, 임시뷰를 만들면 SQL문이 사용가능하다다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fd663",
   "metadata": {},
   "source": [
    "메타데이터 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075d82a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listColumns('us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca832e0",
   "metadata": {},
   "source": [
    "Distance가 1000이상인 것 조회   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef16e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "          FROM us_delay_flights_tbl\n",
    "          WHERE distance > 1000\n",
    "          ORDER BY distance DESC \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c4716b",
   "metadata": {},
   "source": [
    "SFO(샌프란시스코)와 ORD(시카고)간 2시간 이상 지연이 있었던 항공편 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "669677f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "          FROM us_delay_flights_tbl\n",
    "          WHERE delay > 120 and origin == 'SFO' and destination == 'ORD'\n",
    "          ORDER BY delay DESC\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a69e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  long delays|\n",
      "|  305|   ABE|        ATL|  long delays|\n",
      "|  275|   ABE|        ATL|  long delays|\n",
      "|  257|   ABE|        ATL|  long delays|\n",
      "|  247|   ABE|        ATL|  long delays|\n",
      "|  247|   ABE|        DTW|  long delays|\n",
      "|  219|   ABE|        ORD|  long delays|\n",
      "|  211|   ABE|        ATL|  long delays|\n",
      "|  197|   ABE|        DTW|  long delays|\n",
      "|  192|   ABE|        ORD|  long delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    "          CASE\n",
    "            WHEN delay > 360 then 'very long delays'\n",
    "            WHEN delay >= 120 and delay <= 360 then 'long delays'\n",
    "            WHEN delay >= 60 and delay < 120 then 'short delays'\n",
    "            WHEN delay > 0 and delay < 60 then 'Tolerable delays'\n",
    "            WHEN delay = 0 then 'No delays'\n",
    "            ELSE 'Early'\n",
    "          END AS Flight_Delays\n",
    "          FROM us_delay_flights_tbl\n",
    "          ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed06c61",
   "metadata": {},
   "source": [
    "임시 뷰가 아닌 파이썬 데이터프레임 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed3392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, desc\n",
    "(df\n",
    " .select('distance','origin','destination')\n",
    " .where(col('distance')>1000)\n",
    " .orderBy(desc('distance'))\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce92459",
   "metadata": {},
   "source": [
    "<h3> SQL 데이터베이스와 테이블 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a01329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터베이스 생성\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a3de4",
   "metadata": {},
   "source": [
    "생성한 데이터베이스 안에 테이블을 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc727d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"date STRING, delay INT, distance INT, origin STRING, destination STRING\"\n",
    "flights_df = spark.read.csv(filepath, header=True, schema=schema)\n",
    "flights_df.write.saveAsTable('managed_us_delay_flights_tbl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b77b5e",
   "metadata": {},
   "source": [
    "항공사 데이터에서 전역/일반 임시 뷰 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "855559cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sfo = spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "                      FROM us_delay_flights_tbl\n",
    "                      WHERE origin = 'SFO'\"\"\")\n",
    "df_jfk = spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "                      FROM us_delay_flights_tbl\n",
    "                      WHERE origin = 'JFK'\"\"\")\n",
    "\n",
    "# create a temporary and global temporary view\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816b1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전역 임시 뷰 접근\n",
    "# 방법1\n",
    "spark.read.table(\"global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)\n",
    "# 방법2\n",
    "spark.sql(\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b34d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 일반 임시 뷰 접근\n",
    "# 방법1\n",
    "spark.read.table(\"us_origin_airport_JFK_tmp_view\").show(5)\n",
    "# 방법2\n",
    "spark.sql(\"SELECT * FROM us_origin_airport_JFK_tmp_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뷰 드롭\n",
    "spark.catalog.dropGlobalTempView(\"us_origin_airport_SFO_global_tmp_view\")\n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460cecf",
   "metadata": {},
   "source": [
    "파키에 파일을 데이터프레임으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6034f217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|    1|\n",
      "|       United States|            Ireland|  264|\n",
      "|       United States|              India|   69|\n",
      "|               Egypt|      United States|   24|\n",
      "|   Equatorial Guinea|      United States|    1|\n",
      "|       United States|          Singapore|   25|\n",
      "|       United States|            Grenada|   54|\n",
      "|          Costa Rica|      United States|  477|\n",
      "|             Senegal|      United States|   29|\n",
      "|       United States|   Marshall Islands|   44|\n",
      "|              Guyana|      United States|   17|\n",
      "|       United States|       Sint Maarten|   53|\n",
      "|               Malta|      United States|    1|\n",
      "|             Bolivia|      United States|   46|\n",
      "|            Anguilla|      United States|   21|\n",
      "|Turks and Caicos ...|      United States|  136|\n",
      "|       United States|        Afghanistan|    2|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Italy|      United States|  390|\n",
      "|       United States|             Russia|  156|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = 'part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet'\n",
    "df=spark.read.format('parquet').load(filepath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467daceb",
   "metadata": {},
   "source": [
    "JSON 파일을 데이터프레임으로 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a62335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = '2015-summary.json'\n",
    "df = spark.read.format('json').load(filepath)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6f7f2",
   "metadata": {},
   "source": [
    "데이터 프레임을 json파일로 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d92ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = (df.write.format('json')\n",
    " .mode('overwrite')\n",
    " .option('compression','snappy')\n",
    " .save('저장 경로')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79186543",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
