{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5bf6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         #.master(\"local[10]\") # 이그제큐터 코어 갯수 설정 하기, 해당 코드는 1개의 이그제큐터에 10개의 코어를 할당\n",
    "         .appName(\"Example\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110a6afc",
   "metadata": {},
   "source": [
    "캐싱과 영속화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cbd5831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3243a911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|square|\n",
      "+---+------+\n",
      "|  0|     0|\n",
      "|  1|     1|\n",
      "|  2|     4|\n",
      "|  3|     9|\n",
      "|  4|    16|\n",
      "|  5|    25|\n",
      "|  6|    36|\n",
      "|  7|    49|\n",
      "|  8|    64|\n",
      "|  9|    81|\n",
      "+---+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\") )\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5b6c7",
   "metadata": {},
   "source": [
    "캐시 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "865fc82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5043022632598877\n"
     ]
    }
   ],
   "source": [
    "df.cache() \n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce81c776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11701583862304688\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d558cb",
   "metadata": {},
   "source": [
    "캐시에서 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7457e332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, square: bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66439a6",
   "metadata": {},
   "source": [
    "영속화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "015e13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac3e0de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(1 * 10000000).toDF(\"id\").withColumn(\"square\", col(\"id\") * col(\"id\") *2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a3bd7ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3100874423980713\n"
     ]
    }
   ],
   "source": [
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3d5ddae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13684463500976562\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df.count()\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ed5018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183d619",
   "metadata": {},
   "source": [
    "셔플 소트 머지 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d5c88582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test App\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "523a5ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"C:/Users/jaehy/anaconda3/python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"C:/Users/jaehy/anaconda3/python.exe\"\n",
    "# 아래의 설정으로 자동으로 SMJ가 시행됨.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # 자동으로 SMJ 시행\n",
    "\n",
    "# state, item column 생성을 위한 dict\n",
    "states_dict = {0:\"AZ\", 1:\"CO\", 2:\"CA\", 3:\"TX\", 4:\"NY\", 5:\"MI\" }\n",
    "items_dict = {0:\"SKU-0\", 1:\"SKU-1\", 2:\"SKU-2\", 3:\"SKU-3\", 4:\"SKU-4\", 5:\"SKU-5\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f8463801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usersDF 생성\n",
    "usersDF = spark.range(1 * 10000000).rdd.map(lambda x: (str(x[0]),\n",
    "                                                       \"user_\"+str(x[0]),\n",
    "                                                       \"user_\"+str(x[0])+\"@databricks.com\",\n",
    "                                                       states_dict[random.choice(range(6))])\n",
    "                                            ).toDF([\"uid\", \"login\", \"email\", \"user_state\"])\n",
    "#usersDF.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f986f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.range(1 * 10000000).rdd.map(lambda x: (x[0],\n",
    "                                                        x[0],\n",
    "                                                        random.choice(range(10001)),\n",
    "                                                        10 * x[0] * 0.2,\n",
    "                                                        states_dict[random.choice(range(6))],\n",
    "                                                        items_dict[random.choice(range(6))])\n",
    "                                             ).toDF([\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0798b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc2911e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#1436L], [cast(uid#1424 as bigint)], Inner\n",
      "   :- Sort [users_id#1436L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(users_id#1436L, 200), ENSURE_REQUIREMENTS, [plan_id=1135]\n",
      "   :     +- Filter isnotnull(users_id#1436L)\n",
      "   :        +- Scan ExistingRDD[transaction_id#1434L,quantity#1435L,users_id#1436L,amount#1437,state#1438,items#1439]\n",
      "   +- Sort [cast(uid#1424 as bigint) ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(cast(uid#1424 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=1136]\n",
      "         +- Filter isnotnull(uid#1424)\n",
      "            +- Scan ExistingRDD[uid#1424,login#1425,email#1426,user_state#1427]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "13bcb6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
